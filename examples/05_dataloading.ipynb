{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octo Dataloading Examples\n",
    "\n",
    "This notebook will walk you through some of the primary features of the Octo dataloader. Data is, after all, the most important part of any machine learning pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Open X-Embodiment Data\n",
    "\n",
    "The [Open X-Embodiment (OXE)](https://robotics-transformer-x.github.io/) project was a massive cross-instutition data collection collaboration the likes of which robot learning has never seen before. The resulting dataset includes 22 different robots demonstrating 527 skills and totals over 1 million trajectories. However, as we found throughout the course of the Octo project, simply loading such a diverse set of robot data is no small feat. We hope that the `octo.data` pipeline can help kickstart anyone who hopes to take advantage of the massive collection of robot demonstrations that is OXE!\n",
    "\n",
    "### Minimum working example to load a single OXE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 12:13:35.835940: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-02 12:13:35.874234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 12:13:35.874278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 12:13:35.875301: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 12:13:35.881420: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-02 12:13:35.882473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 12:13:36.550533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/erbauer/miniforge3/envs/octo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-02 12:13:37.991969: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
      "2024-08-02 12:13:41.129862: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-08-02 12:13:41.228391: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x7fdb6d0e7400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7fdb6d0e7400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x7fdb6d0e7400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7fdb6d0e7400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _gcd_import at 0x7fdb6d0e7400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x7fdb6d0e7400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:09,  1.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mocto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_single_dataset\n\u001b[1;32m      5\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m make_oxe_dataset_kwargs(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# see octo/data/oxe/oxe_dataset_configs.py for available datasets\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# (this is a very small one for faster loading)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://gresearch/robotics\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmake_single_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# load the train split\u001b[39;00m\n\u001b[1;32m     14\u001b[0m iterator \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39miterator()\n",
      "File \u001b[0;32m~/octo_experiments/octo/octo/data/dataset.py:482\u001b[0m, in \u001b[0;36mmake_single_dataset\u001b[0;34m(dataset_kwargs, train, traj_transform_kwargs, frame_transform_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_single_dataset\u001b[39m(\n\u001b[1;32m    468\u001b[0m     dataset_kwargs: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m     frame_transform_kwargs: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    473\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dl\u001b[38;5;241m.\u001b[39mDLataset:\n\u001b[1;32m    474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a single dataset from kwargs. Returns a dataset of trajectories.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        frame_transform_kwargs: kwargs passed to 'get_frame_transforms'.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     dataset, dataset_statistics \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset_from_rlds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m apply_trajectory_transforms(dataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtraj_transform_kwargs, train\u001b[38;5;241m=\u001b[39mtrain)\n\u001b[1;32m    488\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m apply_frame_transforms(dataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mframe_transform_kwargs, train\u001b[38;5;241m=\u001b[39mtrain)\n",
      "File \u001b[0;32m~/octo_experiments/octo/octo/data/dataset.py:402\u001b[0m, in \u001b[0;36mmake_dataset_from_rlds\u001b[0;34m(name, data_dir, action_encoding, train, standardize_fn, shuffle, image_obs_keys, depth_obs_keys, proprio_obs_key, language_key, action_proprio_normalization_type, dataset_statistics, force_recompute_dataset_statistics, action_normalization_mask, filter_functions, skip_norm, ignore_errors, num_parallel_reads, num_parallel_calls)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# tries to load from cache, otherwise computes on the fly\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     dataset_statistics \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhash_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproprio_obs_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43mModuleSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandardize_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstandardize_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModuleSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_functions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_recompute_dataset_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    416\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load statistics for dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/octo_experiments/octo/octo/data/utils/data_utils.py:147\u001b[0m, in \u001b[0;36mget_dataset_statistics\u001b[0;34m(dataset, hash_dependencies, save_dir, force_recompute)\u001b[0m\n\u001b[1;32m    145\u001b[0m num_transitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    146\u001b[0m num_trajectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m    148\u001b[0m     dataset\u001b[38;5;241m.\u001b[39miterator(),\n\u001b[1;32m    149\u001b[0m     total\u001b[38;5;241m=\u001b[39mcardinality \u001b[38;5;28;01mif\u001b[39;00m cardinality \u001b[38;5;241m!=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mUNKNOWN_CARDINALITY \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m ):\n\u001b[1;32m    151\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(traj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprio\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m traj:\n",
      "File \u001b[0;32m~/miniforge3/envs/octo/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/octo/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:4733\u001b[0m, in \u001b[0;36mNumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4730\u001b[0m     numpy\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   4731\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(to_numpy, \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/octo/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/octo/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/octo/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3024\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3023\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# minimum working example to load a single OXE dataset\n",
    "from octo.data.oxe import make_oxe_dataset_kwargs\n",
    "from octo.data.dataset import make_single_dataset\n",
    "\n",
    "dataset_kwargs = make_oxe_dataset_kwargs(\n",
    "    # see octo/data/oxe/oxe_dataset_configs.py for available datasets\n",
    "    # (this is a very small one for faster loading)\n",
    "    \"austin_buds_dataset_converted_externally_to_rlds\",\n",
    "    # can be local or on cloud storage (anything supported by TFDS)\n",
    "    # \"/path/to/base/oxe/directory\",\n",
    "    \"gs://gresearch/robotics\",\n",
    ")\n",
    "dataset = make_single_dataset(dataset_kwargs, train=True) # load the train split\n",
    "iterator = dataset.iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_single_dataset yields entire trajectories\n",
    "traj = next(iterator)\n",
    "print(\"Top-level keys: \", traj.keys())\n",
    "print(\"Observation keys: \", traj[\"observation\"].keys())\n",
    "print(\"Task keys: \", traj[\"task\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "traj = next(iterator)\n",
    "images = traj[\"observation\"][\"image_primary\"]\n",
    "# should be: (traj_len, window_size, height, width, channels)\n",
    "# (window_size defaults to 1)\n",
    "print(images.shape)  \n",
    "Image.fromarray(np.concatenate(images.squeeze()[-5:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should set these much higher in practice (as large as your memory can hold!)\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# turning a dataset of trajectories into a training-ready batched dataset\n",
    "train_dataset = (\n",
    "    dataset.flatten() # flattens trajectories into individual frames\n",
    "    .shuffle(SHUFFLE_BUFFER_SIZE) # shuffles the frames\n",
    "    .batch(BATCH_SIZE) # batches the frames\n",
    ")\n",
    "batch = next(train_dataset.iterator())\n",
    "images = batch[\"observation\"][\"image_primary\"]\n",
    "# should be: (batch_size, window_size, height, width, channels)\n",
    "print(images.shape)\n",
    "Image.fromarray(np.concatenate(images.squeeze()[:5], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a training-ready OXE mix\n",
    "\n",
    "In reality, you're probably going to want to mix multiple datasets together, as well as use other transformations such as resizing, augmentation, windowing, etc. This section will show you how to get a proper OXE mix up and running, as well as demonstrate additional `octo.data` features for more realistic use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octo.data.oxe import make_oxe_dataset_kwargs_and_weights\n",
    "from octo.data.dataset import make_interleaved_dataset\n",
    "\n",
    "dataset_kwargs_list, sample_weights = make_oxe_dataset_kwargs_and_weights(\n",
    "    # you can pass your own list of dataset names and sample weights here, but we've\n",
    "    # also provided a few named mixes for convenience. The Octo model was trained\n",
    "    # using the \"oxe_magic_soup\" mix.\n",
    "    \"rtx\",\n",
    "    # can be local or on cloud storage (anything supported by TFDS)\n",
    "    \"gs://gresearch/robotics\",\n",
    "    # let's get a wrist camera!\n",
    "    load_camera_views=(\"primary\", \"wrist\"),\n",
    ")\n",
    "\n",
    "# see `octo.data.dataset.make_dataset_from_rlds` for the meaning of these kwargs\n",
    "dataset_kwargs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_interleaved_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# each element of `dataset_kwargs_list` can be used with `make_single_dataset`, but let's\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# use the more powerful `make_interleaved_dataset` to combine them for us!\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmake_interleaved_dataset\u001b[49m(\n\u001b[1;32m      7\u001b[0m     dataset_kwargs_list,\n\u001b[1;32m      8\u001b[0m     sample_weights,\n\u001b[1;32m      9\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     balance_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# unlike our manual shuffling above, `make_interleaved_dataset` will shuffle\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# the JPEG-encoded images, so you should be able to fit a much larger buffer size\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     shuffle_buffer_size\u001b[38;5;241m=\u001b[39mSHUFFLE_BUFFER_SIZE,\n\u001b[1;32m     14\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# see `octo.data.dataset.apply_trajectory_transforms` for full documentation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# of these configuration options\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     traj_transform_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     18\u001b[0m         goal_relabeling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# let's get some goal images\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# let's get some history\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         action_horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,  \u001b[38;5;66;03m# let's get some future actions for action chunking\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         subsample_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# subsampling long trajectories improves shuffling a lot\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     ),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# see `octo.data.dataset.apply_frame_transforms` for full documentation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# of these configuration options\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     frame_transform_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# let's apply some basic image augmentations -- see `dlimp.transforms.augment_image`\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# for full documentation of these configuration options\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         image_augment_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     29\u001b[0m             primary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     30\u001b[0m                 augment_order\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_resized_crop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_brightness\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m                 random_resized_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], ratio\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m1.1\u001b[39m]),\n\u001b[1;32m     32\u001b[0m                 random_brightness\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.1\u001b[39m],\n\u001b[1;32m     33\u001b[0m             )\n\u001b[1;32m     34\u001b[0m         ),\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# providing a `resize_size` is highly recommended for a mixed dataset, otherwise\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# datasets with different resolutions will cause errors\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         resize_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     38\u001b[0m             primary\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     39\u001b[0m             wrist\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m     40\u001b[0m         ),\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# If parallelism options are not provided, they will default to tf.Data.AUTOTUNE.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# However, we would highly recommend setting them manually if you run into issues\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# with memory or dataloading speed. Frame transforms are usually the speed\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# bottleneck (due to image decoding, augmentation, and resizing), so you can set\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# this to a very high value if you have a lot of CPU cores. Keep in mind that more\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# parallel calls also use more memory, though.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     48\u001b[0m     ),\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Same spiel as above about performance, although trajectory transforms and data reading\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# are usually not the speed bottleneck. One reason to manually set these is if you want\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# to reduce memory usage (since autotune may spawn way more threads than necessary).\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     traj_transform_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     53\u001b[0m     traj_read_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Another performance knob to tune is the number of batches to prefetch -- again,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# the default of tf.data.AUTOTUNE can sometimes use more memory than necessary.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m iterator \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39miterator(prefetch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_interleaved_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# each element of `dataset_kwargs_list` can be used with `make_single_dataset`, but let's\n",
    "# use the more powerful `make_interleaved_dataset` to combine them for us!\n",
    "dataset = make_interleaved_dataset(\n",
    "    dataset_kwargs_list,\n",
    "    sample_weights,\n",
    "    train=True,\n",
    "    balance_weights=True,\n",
    "    # unlike our manual shuffling above, `make_interleaved_dataset` will shuffle\n",
    "    # the JPEG-encoded images, so you should be able to fit a much larger buffer size\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # see `octo.data.dataset.apply_trajectory_transforms` for full documentation\n",
    "    # of these configuration options\n",
    "    traj_transform_kwargs=dict(\n",
    "        goal_relabeling_strategy=\"uniform\",  # let's get some goal images\n",
    "        window_size=2,  # let's get some history\n",
    "        action_horizon=4,  # let's get some future actions for action chunking\n",
    "        subsample_length=100,  # subsampling long trajectories improves shuffling a lot\n",
    "    ),\n",
    "    # see `octo.data.dataset.apply_frame_transforms` for full documentation\n",
    "    # of these configuration options\n",
    "    frame_transform_kwargs=dict(\n",
    "        # let's apply some basic image augmentations -- see `dlimp.transforms.augment_image`\n",
    "        # for full documentation of these configuration options\n",
    "        image_augment_kwargs=dict(\n",
    "            primary=dict(\n",
    "                augment_order=[\"random_resized_crop\", \"random_brightness\"],\n",
    "                random_resized_crop=dict(scale=[0.8, 1.0], ratio=[0.9, 1.1]),\n",
    "                random_brightness=[0.1],\n",
    "            )\n",
    "        ),\n",
    "        # providing a `resize_size` is highly recommended for a mixed dataset, otherwise\n",
    "        # datasets with different resolutions will cause errors\n",
    "        resize_size=dict(\n",
    "            primary=(256, 256),\n",
    "            wrist=(128, 128),\n",
    "        ),\n",
    "        # If parallelism options are not provided, they will default to tf.Data.AUTOTUNE.\n",
    "        # However, we would highly recommend setting them manually if you run into issues\n",
    "        # with memory or dataloading speed. Frame transforms are usually the speed\n",
    "        # bottleneck (due to image decoding, augmentation, and resizing), so you can set\n",
    "        # this to a very high value if you have a lot of CPU cores. Keep in mind that more\n",
    "        # parallel calls also use more memory, though.\n",
    "        num_parallel_calls=64,\n",
    "    ),\n",
    "    # Same spiel as above about performance, although trajectory transforms and data reading\n",
    "    # are usually not the speed bottleneck. One reason to manually set these is if you want\n",
    "    # to reduce memory usage (since autotune may spawn way more threads than necessary).\n",
    "    traj_transform_threads=16,\n",
    "    traj_read_threads=16,\n",
    ")\n",
    "\n",
    "# Another performance knob to tune is the number of batches to prefetch -- again,\n",
    "# the default of tf.data.AUTOTUNE can sometimes use more memory than necessary.\n",
    "iterator = dataset.iterator(prefetch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phew, that was a lot of configuration! Let's see what we got.\n",
    "batch = next(iterator)\n",
    "print(\"Top-level keys: \", batch.keys())\n",
    "# should now have \"image_primary\" and \"image_wrist\"!\n",
    "print(\"Observation keys: \", batch[\"observation\"].keys())\n",
    "# should also have \"image_primary\" and \"image_wrist\", corresponding to future goal images\n",
    "print(\"Task keys: \", batch[\"task\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "images_primary = batch[\"observation\"][\"image_primary\"]\n",
    "images_wrist = batch[\"observation\"][\"image_wrist\"]\n",
    "# should be: (batch_size, window_size (now 2), height, width, channels)\n",
    "print(images_primary.shape)\n",
    "print(images_wrist.shape)\n",
    "actions = batch[\"action\"]\n",
    "# should be: (batch_size, window_size, action_horizon, action_dim)\n",
    "print(actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize a window of primary images\n",
    "display(Image.fromarray(np.concatenate(images_primary[0], axis=1)))\n",
    "# now a window of wrist images -- many datasets don't have wrist images,\n",
    "# so this will often be black\n",
    "display(Image.fromarray(np.concatenate(images_wrist[0], axis=1)))\n",
    "# pad_mask_dict also tells you which keys should be treated as padding\n",
    "# (e.g., if the wrist camera is black, the corresponding pad_mask_dict entry is False)\n",
    "print(batch[\"observation\"][\"pad_mask_dict\"][\"image_wrist\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the \"task\" dict: it should now have both goal\n",
    "# images and language instructions!\n",
    "goal_primary = batch[\"task\"][\"image_primary\"]\n",
    "goal_wrist = batch[\"task\"][\"image_wrist\"]\n",
    "language_instruction = batch[\"task\"][\"language_instruction\"]\n",
    "display(Image.fromarray(goal_primary[0]))\n",
    "display(Image.fromarray(goal_wrist[0]))\n",
    "print(language_instruction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octo-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
